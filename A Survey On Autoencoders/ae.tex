\documentclass[11pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\date{}
\usepackage{titlesec}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{subfig}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage[font={small,it}]{caption}
\usepackage[top=0.5in, bottom=1in, left=1in, right=1.in]{geometry}

\usepackage{titlesec}

\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}


\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}
\DeclareMathOperator{\EX}{\mathbb{E}}
\begin{document}
\title{\centerline{\rule{15cm}{2pt}} \textbf{A Survey On Autoencoders}\\\centerline{\rule{15cm}{0.4pt}}}
\author{
  BILICI, M. Åžafak\\
  \texttt{safakk.bilici.2112@gmail.com}}
\maketitle
\textbf{\textit{Abstract--}} \textbf{Autoencoders are an unsupervised learning architectures in neural networks. Theys are commonly used in Deep Learning tasks; such as generative models, anomaly detection, dimensionality reduction. In this article, we will evaluate theoretical approaches of Autoencoders and see it's extensions.}

\section{1 Introduction}
\hspace*{0.5cm} Autoencoders are an unsupervised learning method. They map the input data into lower dimensional space with encoder $E$, and then maps into same space that have same dimension of input data with decoder $D$.\\
\begin{tikzpicture}

      \node[inner sep=3pt] (z) at (-0.4,-1.15){$\cdots$};
      \node[circle,draw=black,minimum size=0.2cm,inner sep=3pt] (a) at (-1,0){};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (b) at (-1,-0.5){};
      
      \node[] (e) at (0.2,0){};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (f) at (1.3,-0.6){};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (g) at (1.3,-1.1){};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (h) at (1.3,-1.6){};
      
      \node[inner sep=3pt] (z) at (-1,-1.03){$\vdots$};
     
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (c) at (-1,-1.7){};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (d) at (-1,-2.2){};
      \node[] (w) at (-2,-1.1){$\mathbf{x}$};
      \node[inner sep=3pt] (o) at (2.8,-1.15){$\cdots$};
      \node[circle,draw=black,minimum size=0.2cm,inner sep=3pt] (i) at (3.3,0){};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (j) at (3.3,-0.5){};
      \node[inner sep=3pt] (m) at (3.3,-1.03){$\vdots$};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (k) at (3.3,-1.7){};
      \node[circle,draw=black,minimum size=0.1cm,inner sep=3pt] (l) at (3.3,-2.2){};
      
	  \node[inner sep=3pt] (m) at (1.3,-0.1){$\vdots$};      
	  \node[inner sep=3pt] (m) at (1.3,-1.9){$\vdots$};      
      
      
      \tiny
      \node[] (n) at (4.5,-1.03){$L(\mathbf{x},E(D(\mathbf{x})))$};
      
      \node[inner sep=3pt] (p) at (-0.05,-2.3){$E$};
      \node[inner sep=3pt] (r) at (2.4,-2.3){$D$};
      
      \draw[->] (w) -- (a);
      \draw[->] (w) -- (b);
      \draw[->] (w) -- (c);
      \draw[->] (w) -- (d);
      \draw[dashed,->] (a) -- (f);
      \draw[dashed,->] (a) -- (g);
      \draw[dashed,->] (a) -- (h);
      \draw[dashed,->] (b) -- (f);
      \draw[dashed,->] (b) -- (g);
      \draw[dashed,->] (b) -- (h);
      \draw[dashed,->] (c) -- (f);
      \draw[dashed,->] (c) -- (g);
      \draw[dashed,->] (c) -- (h);
      \draw[dashed,->] (d) -- (f);
      \draw[dashed,->] (d) -- (g);
      \draw[dashed,->] (d) -- (h);
      \draw[dashed,->] (f) -- (i);
      \draw[dashed,->] (f) -- (j);
      \draw[dashed,->] (f) -- (k);
      \draw[dashed,->] (f) -- (l);
      \draw[dashed,->] (g) -- (i);
      \draw[dashed,->] (g) -- (j);
      \draw[dashed,->] (g) -- (k);
      \draw[dashed,->] (g) -- (l);
      \draw[dashed,->] (h) -- (i);
      \draw[dashed,->] (h) -- (j);
      \draw[dashed,->] (h) -- (k);
      \draw[dashed,->] (h) -- (l);
      \draw[->] (i) -- (n);
      \draw[->] (j) -- (n);
      \draw[->] (k) -- (n);
      \draw[->] (l) -- (n);
    \end{tikzpicture}
The main idea behind Autoencoders is to attempt to copy its input to its output. The input layer is fed with input vector $\mathbf{x}$ and the loss is calculated at output layer between $\mathbf{x}$ and $E(D(\mathbf{x}))$, in other words the loss is $L(\mathbf{x},E(D(\mathbf{x})))$. It measures difference between our original input and the consequent reconstruction.  We named the middle layer, that is connection between encoder $E$ and decoder $D$, as the "bottleneck". We can denote our output of bottleneck as $\mathbf{h} = E(\mathbf{x})$ and denote our output as $\mathbf{\hat{x}} = D(\mathbf{h}) = D(E(\mathbf{x}))$. We can define our encoder and decoder as conditional probability density function that are $p_{encoder}(\mathbf{h} | \mathbf{x})$ and $p_{decoder}(\mathbf{\hat{x}} | \mathbf{h})$. \\
The loss function is named reconstruction loss which is $L(\mathbf{\hat{x}},\mathbf{x})$. We can treat the process as a feedforward networks; the loss can be minimized via mini-batch statistics following gradients computed by backpropagation algorithm,
$$\min\limits_{\theta} L = \nabla_\theta L(\mathbf{x}, E(D(\mathbf{x}))) =\nabla_\theta L(\mathbf{x}, \mathbf{\hat{x}}) $$
The bottleneck is the key of the effectiveness of Autoencoders. We map our input vector to bottleneck: the bottleneck keeps the 'latent informations' of input $\mathbf{x}$. The network represents input but in lower dimensions. In other words, it behaves like a approximative compression algorithm. The encoding parameters are learned in training process. Then we map bottleneck information $\mathbf{h}$ into same dimension as input $\mathbf{x}$. Then, this procedure can be seen as approximative extracting compressed latent information.

\section{2 Undercomplete Autoencoders}	
\hspace*{0.5cm} The simplest idea behind autoencoders is the decreasing the number of nodes through the hidden layers before bottleneck. An autoencoder that has dimension less than the input $\mathbf{x}$ is called undercomplete autoencoder. When we minimize the reconstruction error, autoencoder learns to represent latent attributes of input data with lower dimensions than input $\mathbf{x}$'s. This procedure is same as in Principal Component Analysis (PCA) but in non-linear way. When decoder is linear and the loss $L(\mathbf{\hat{x},\mathbf{x}})$ is the $L^2$ error, an autocomplete autoencoder learns to span the same subspace as PCA. When autoencoder has non-linear activations, then autoencoder becomes more powerful and generalized in dimensionality reduction, it becames non-linear version of PCA.

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.75]
  \begin{axis}[%
      title={},
      xlabel={},
      ylabel={},
      yticklabels={,,}
    ]
    \addplot {x^3 - x^2 }; 
    \addplot { 20 * x};
    \addlegendentry{Autoencoder}
	\addlegendentry{PCA}
  \end{axis}
\end{tikzpicture}
\caption{Encoding of PCA and Autoencoder.}
\end{figure}
\subsection{2.1 Problem of Autoencoders}
When we said that main idea behind autoencoders is to copy input to its output, the key idea is that not to copy without extracting useful informations about the distribution of the data. Autoencoders are allowed too much capacity, easy to be trained to the copying the task with learning anything useful about the dataset. So we need to penalize those autoencoders.

\section{3 Regularized Autoencoders}
As we said, 
\end{document}
